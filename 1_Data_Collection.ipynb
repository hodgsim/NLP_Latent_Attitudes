{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection  \n",
    "  \n",
    "In order to calculate reasonably stable embedding vectors using our NN, we'll need to collect a sufficient amount of data. For this exercise, we'll use only data that is free and publicly accessible. It is always worth checking the Terms and Conditions of any site before attempting to scrape data, and even if scraping is allowed, then it is good practice to limit the requests in some way to that the site is not swamped.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load packages that we'll need\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "## 1) Classic Literature  \n",
    "\n",
    "The text of many books in the public domain can be obtained from Gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_reader(urls):\n",
    "    \"\"\"\n",
    "    Parses the HTML of a given page using Beautiful Soup, extracts the text of the book as a string,\n",
    "    and then tokenizes that string into individual words\n",
    "    \n",
    "    Args:\n",
    "        urls : A list containing urls for each book to process\n",
    "        \n",
    "    Returns:\n",
    "        (nltk object) Combined text of all the books in the list\n",
    "    \"\"\"\n",
    "    final = ''\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "        page = urllib.request.urlopen(url)\n",
    "        book = BeautifulSoup(page, 'html.parser')\n",
    "        raw = book.get_text()\n",
    "        final += raw.lower()\n",
    "    tokens = nltk.word_tokenize(final)\n",
    "    text = nltk.Text(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Urls for a selection of books by female authors that we want to train on\n",
    "urls_female = ['https://www.gutenberg.org/files/1342/1342-0.txt', # Jane Austen\n",
    "        'https://www.gutenberg.org/files/158/158-0.txt', \n",
    "        'http://www.gutenberg.org/cache/epub/161/pg161.txt', \n",
    "        'http://www.gutenberg.org/cache/epub/105/pg105.txt', \n",
    "        'https://www.gutenberg.org/files/121/121-0.txt', \n",
    "        'http://www.gutenberg.org/cache/epub/946/pg946.txt', \n",
    "        'https://www.gutenberg.org/files/1212/1212-0.txt', \n",
    "        'http://www.gutenberg.org/cache/epub/5670/pg5670.txt', # Virginia Woolf\n",
    "        'https://www.gutenberg.org/files/144/144-0.txt', \n",
    "        'https://www.gutenberg.org/files/1245/1245-0.txt', \n",
    "        'http://www.gutenberg.org/cache/epub/29220/pg29220.txt',\n",
    "        'https://www.gutenberg.org/files/57050/57050-0.txt', \n",
    "        'http://www.gutenberg.org/cache/epub/1260/pg1260.txt', # Charlotte Bronte\n",
    "        'http://www.gutenberg.org/cache/epub/9182/pg9182.txt',\n",
    "        'https://www.gutenberg.org/files/30486/30486-0.txt', \n",
    "        'https://www.gutenberg.org/files/1028/1028-0.txt', \n",
    "        'http://www.gutenberg.org/cache/epub/54254/pg54254.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/768/pg768.txt', # Emily Bronte\n",
    "        'http://www.gutenberg.org/cache/epub/145/pg145.txt', # George Eliot\n",
    "        'http://www.gutenberg.org/cache/epub/550/pg550.txt',\n",
    "        'https://www.gutenberg.org/files/6688/6688-0.txt',\n",
    "        'https://www.gutenberg.org/files/28289/28289-0.txt',\n",
    "        'https://www.gutenberg.org/files/507/507-0.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/7469/pg7469.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/2165/pg2165.txt',\n",
    "        'https://www.gutenberg.org/files/24020/24020-0.txt',\n",
    "        'https://www.gutenberg.org/files/40882/40882-0.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/2171/pg2171.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/17780/pg17780.txt']\n",
    "\n",
    "# Urls for a selection of books by male authors that we want to train on\n",
    "urls_male = ['https://www.gutenberg.org/files/46/46-0.txt', # Charles Dickens\n",
    "        'https://www.gutenberg.org/files/98/98-0.txt',\n",
    "        'https://www.gutenberg.org/files/1400/1400-0.txt',\n",
    "        'https://www.gutenberg.org/files/786/786-0.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/730/pg730.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/19337/pg19337.txt',\n",
    "        'https://www.gutenberg.org/files/766/766-0.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/1023/pg1023.txt',\n",
    "        'https://www.gutenberg.org/files/580/580-0.txt',\n",
    "        'https://www.gutenberg.org/files/74/74-0.txt', # Mark Twain\n",
    "        'https://www.gutenberg.org/files/76/76-0.txt',\n",
    "        'https://www.gutenberg.org/files/86/86-0.txt',\n",
    "        'https://www.gutenberg.org/files/1837/1837-0.txt',\n",
    "        'https://www.gutenberg.org/files/3176/3176-0.txt',\n",
    "        'https://www.gutenberg.org/files/245/245-0.txt',\n",
    "        'https://www.gutenberg.org/files/102/102-0.txt',\n",
    "        'https://www.gutenberg.org/files/3186/3186-0.txt',\n",
    "        'https://www.gutenberg.org/files/3177/3177-0.txt',\n",
    "        'https://www.gutenberg.org/files/8525/8525-0.txt',\n",
    "        'https://www.gutenberg.org/files/2701/2701-0.txt', # Herman Melville\n",
    "        'http://www.gutenberg.org/cache/epub/11231/pg11231.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/21816/pg21816.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/15859/pg15859.txt',\n",
    "        'https://www.gutenberg.org/files/1900/1900-0.txt',\n",
    "        'http://www.gutenberg.org/cache/epub/12384/pg12384.txt',\n",
    "        'https://www.gutenberg.org/files/805/805-0.txt', # F. Scott Fitzgerald\n",
    "        'http://gutenberg.net.au/ebooks02/0200041.txt',\n",
    "        'http://gutenberg.net.au/ebooks03/0301261.txt',\n",
    "        'http://gutenberg.net.au/ebooks01/0100021.txt', # George Orwell\n",
    "        'http://gutenberg.net.au/ebooks03/0300011.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0200151.txt',\n",
    "        'http://gutenberg.net.au/ebooks01/0100011.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0200031.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0201111.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0200391.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0200141.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0200021.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0200011.txt',\n",
    "        'http://gutenberg.net.au/ebooks02/0200051.txt',\n",
    "        'http://gutenberg.net.au/ebooks01/0100171.txt']\n",
    "\n",
    "# Save the data as a text file\n",
    "text = book_reader(urls_female)\n",
    "with open(\"Female_Authors.txt\", 'w') as file:\n",
    "    for item in text:\n",
    "        file.write(\"{} \".format(item))\n",
    "        \n",
    "text = book_reader(urls_male)\n",
    "with open(\"Male_Authors.txt\", 'w') as file:\n",
    "    for item in text:\n",
    "        file.write(\"{} \".format(item))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Movie Scripts  \n",
    "  \n",
    "There are several sites that publish movie scripts. For this exercise, we'll use xxxxxxxxxx.com, which includes a two page index of all movie scripts hosted on the site. The first step is to collect that index so that we can create a list of urls, that we'll then loop through in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index of movies is split over two pages. We'll collect the name of the movie, the url for the script,\n",
    "# and the year of release (in case we later want to examine how biases have changed over time)\n",
    "\n",
    "source_page = [\"http://www.xxxxxxxxxx.com/movie.html\", \"http://www.xxxxxxxxxx.com/movie_n-z.html\"]\n",
    "movies = []\n",
    "for source in source_page:\n",
    "    page = urllib.request.urlopen(source)\n",
    "    parsed = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    for a in parsed.find_all('a', href=True):\n",
    "        # only grab links to scripts, not to the imdb entry\n",
    "        if a['href'][:7] == 'scripts':\n",
    "            url = \"http://www.xxxxxxxxxx.com/\"+a['href']\n",
    "            name = unicodedata.normalize(\"NFKD\", a.contents[0])\n",
    "            year = unicodedata.normalize(\"NFKD\", a.contents[0].next_element[-4:])\n",
    "            movies.append([name,url,year])\n",
    "            \n",
    "# This particular site publishes scripts in either html or as a pdf file. For simplicity, we'll only collect\n",
    "# the html files and won't attempt to scrape the pdfs.\n",
    "\n",
    "# Create a new list that excludes any item where the url ends in 'pdf'\n",
    "movies = [x for x in movies if x[1][-3:].lower() != 'pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list of movies, we can scrape the site to obtain the text of the scripts. We'll build in a delay so that we only make one request every two seconds to avoid hitting theier servers too badly. \n",
    "  \n",
    "The scripts need some pre-processing. Character names are provided before every spoken line, and therefore will be high frequency but also semantically unrelated to the surrounding words, so we'll remove them for the purposes of this analysis. By convention, they are in capitals, and therefore easy to strip out using regex. This will also remove any normal words that are all in capitals (eg \"I\") but the benefit of removing this bias outweighs the loss of capitalized text (which are generally names, background descriptions, and instructions on cinematography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store the scripts, and a list to store any urls where scraping was unsuccessful\n",
    "movie_dict = {}\n",
    "failures = []\n",
    "\n",
    "# Collect the scripts\n",
    "for name,url,year in movies:\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "        time.sleep(2) # delay in order to avoid swamping the website with requests\n",
    "        parsed = BeautifulSoup(page, 'html.parser')\n",
    "        # Remove words that are all caps\n",
    "        script = re.sub(r'\\b[A-Z]+\\b', '', parsed.text)\n",
    "        # Get rid of unnecessary white space and newlines\n",
    "        script = \" \".join(script.split())\n",
    "        movie_dict[name] = [script,year]\n",
    "        # hokey counter, just to keep track of where we are\n",
    "        print(name[:1], end = \"\")\n",
    "    except:\n",
    "        failures.append(url)\n",
    "        \n",
    "# Save the dictionary\n",
    "with open('Movie_Script_dict.txt', 'w') as file:\n",
    "     file.write(json.dumps(movie_dict))\n",
    "        \n",
    "# Save a text file containing all the scripts       \n",
    "with open(\"Movie_Scripts.txt\", 'w') as file:\n",
    "    file.write(\" \".join(movie_dict[name][0] for name in movie_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Song lyrics  \n",
    "\n",
    "For this exercise, we'll collect the lyrics of the billboard top 100 songs in the UK and the US over the last fifty years.\n",
    "\n",
    "There are many sites that publish song lyrics, however, formats between them vary significantly, and there are several that discourage scraping. This means that even with a list of song titles and artists, there may be significant manual intervention required to reformat the name and find the correct lyrics. This quickly becomes tedious (as you will see below), so we'll only perform the exercise for the UK, and collect the US data from the very interesting repo of walkerkq (https://github.com/walkerkq/), who thankfully has already done it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, create a list of webpages that contain the top 100 songs for each year\n",
    "url_dict = {year : 'https://www.xxxxxxxxxx.com/charts/singles-chart/'+str(year)+'0630/7501' for year in range(1965, 2016)}\n",
    "\n",
    "# Run through the dictionary and check that a page exists for each year:url pair\n",
    "for k, v in url_dict2.items():\n",
    "    try:\n",
    "        urllib.request.urlopen(v)\n",
    "        print(k, 'ok')\n",
    "    except:\n",
    "        print(k, 'error - check format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that the dictionary urls are correct, set up a function to scrape the artist names and song titles from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_artists_and_titles(parsed_page):\n",
    "    '''Scrape a specific page from www.xxxxxxxxxx.com to collect\n",
    "       the artist and title of the top 100 songs in the UK for a given year\n",
    "       \n",
    "       Args:\n",
    "           parsed_page: web page parsed with Beautiful Soup\n",
    "       Returns:\n",
    "           List of (title, artist) tuples'''\n",
    "    artist_list = []\n",
    "    title_list = []\n",
    "    for artist in parsed_page.findAll(\"div\", class_=\"artist\"):\n",
    "        artist_list.append(artist.find('a').contents[0])\n",
    "    for title in parsed_page.findAll(\"div\", class_=\"title\"):\n",
    "        title_list.append(title.find('a').contents[0])\n",
    "    return(list(zip(title_list, artist_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the artist names provided by this website differs from the one used by the site which contains the actual lyrics. The following code is therefore specific to the target lyrics site (ie, it would have to be changed for use with other sites that use different naming conventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lyrics(combined_list):\n",
    "    '''Loop through the list of top 100 songs, format the names correctly\n",
    "       so that the lyrics can be scraped from www.xxxxxxxxxx.com and then\n",
    "       scrape them\n",
    "       \n",
    "       Args:\n",
    "           List of (title, artist) tuples\n",
    "       Returns:\n",
    "           List where List[0] is a string containing successfully scraped lyrics, and\n",
    "           List[1] contains the (title, artist) tuples that were not found'''\n",
    "    \n",
    "    # Replace certain symbols that appear in the list of artist / song names with the\n",
    "    # corresponding symbol from the lyrics site\n",
    "    target_list = []\n",
    "    replacements = {' ':'-', '(':'', ')':'', '&':'and', \"'\":\"\", '?':''}\n",
    "    for n in range(len(combined_list)):\n",
    "        target = '-lyrics-'.join(combined_list[n])\n",
    "        for a,b in replacements.items():\n",
    "            target = target.replace(a,b)\n",
    "        target_list.append(target)\n",
    "    \n",
    "    # With artist collaborations, the convention on metrolyrics.com is that anything after\n",
    "    # the substring 'ft' gets dropped, so we need to update the list to reflect that\n",
    "\n",
    "    breakpoint = '-FT'\n",
    "    for n, item in enumerate(target_list):\n",
    "        target_list[n] = item.split(breakpoint, 1)[0]\n",
    "        \n",
    "    # Then, there are two more transformations that we need\n",
    "    # a) where a song includes a subtitle (denoted by a forward slash) the\n",
    "    # subtitle is ignored, so for example, 'Get over you/Move this mountain'\n",
    "    # by Sophie Ellis Bextor is listed under 'Get over you'\n",
    "    #\n",
    "    # b) The word \"The\" is ignored when it precedes the artist name, so\n",
    "    # \"The Beatles\" are listed under \"Beatles\", but \"Hootie and the Blowfish\"\n",
    "    # remains unchanged\n",
    "    \n",
    "    break_s = \"/\"\n",
    "    break_a = \"THE \"\n",
    "    for n, item in enumerate(target_list):\n",
    "        song = item[0].split(break_s, 1)[0]\n",
    "        if artist1[:4] == break_a:\n",
    "            artist = artist1[4:]\n",
    "        else:\n",
    "            artist = artist1\n",
    "        target_list[n] = [song,artist]\n",
    "    \n",
    "    # Now grab the lyrics for all songs in the year, but store any songs for which\n",
    "    # the formatting is incorrect in a separate list, in case we find another source\n",
    "    # or choose to add them manually\n",
    "    \n",
    "    failures = []\n",
    "    lyrics = []\n",
    "    for n in range(len(target_list)):\n",
    "        lyrics_page = 'http://www.xxxxxxxxxx.com/{}.html'.format(target_list[n].lower())\n",
    "        time.sleep(2) # put in a delay so that we don't hammer their website too badly\n",
    "        try:\n",
    "            page2 = urllib.request.urlopen(lyrics_page)\n",
    "            parsed2 = BeautifulSoup(page2, 'html.parser')\n",
    "            for x in parsed2.findAll(\"p\", class_=\"verse\"):\n",
    "                lyrics.append(x.text)\n",
    "        except:\n",
    "            failures.append(combined_list[n])\n",
    "        \n",
    "    all_lyrics = \" \".join(lyrics).replace('\\n','. ')\n",
    "    result = [all_lyrics, failures]\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run through the dictionary of urls and collect as many lyrics as possible. We'll recycle the dictionary, and keep the year as the key, but replace the URL that we no longer need with a list that contains the lyrics for that year, plus any title/artist combinations for the year that we were unable to obtain from the target lyrics site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrape the target site to collect lyrics\n",
    "for k, v in url_dict.items():\n",
    "    page = urllib.request.urlopen(v)\n",
    "    parsed = BeautifulSoup(page, 'html.parser')\n",
    "    combined = get_artists_and_titles(parsed)\n",
    "    lyrics = get_lyrics(combined)\n",
    "    # replace the URL for the given year with the lyrics of the top 100 songs and a list of songs that we missed\n",
    "    url_dict[k] = lyrics\n",
    "    # this process can take a while, so to keep track, print out when each year has been processed\n",
    "    print(k, ' done')\n",
    "\n",
    "# Save the dictionary\n",
    "with open('UK_lyric_dict.txt', 'w') as file:\n",
    "     file.write(json.dumps(url_dict))\n",
    "        \n",
    "# Save a file containing the lyrics for all years\n",
    "with open(\"UK_lyrics.txt\", 'w') as file:\n",
    "    file.write(\" \".join(url_dict[year][0] for year in url_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, at this stage there are still several issues with the naming convention which means that we have incomplete data. We saved the (title, artist) tuple for anything that couldn't be found on the target site in the dictionary. Check the size of the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each key value pair in the dictionary key = year, value = [lyrics, [(title,artist) that failed]]\n",
    "# Print out how many failures there are for each year and what the overall coverage is\n",
    "\n",
    "missing = 0\n",
    "for year in url_dict.keys():\n",
    "    print(year, len(url_dict[year][1]))\n",
    "    missing += len(url_dict[year][1])\n",
    "print(\"Coverage is now {}%\".format(100 - missing*100/5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try scraping a different site for the songs we are missing\n",
    "\n",
    "Adjust the code so that it will be compatible with a different site, xxxxxxxxxx.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lyrics_2(combined_list):\n",
    "    '''Loop through the list of remaining songs, format the names correctly\n",
    "       so that the lyrics can be scraped from www.xxxxxxxxxx.com and then\n",
    "       scrape them\n",
    "       \n",
    "       Args:\n",
    "           List of (title, artist) tuples\n",
    "       Returns:\n",
    "           List where List[0] is a string containing successfully scraped lyrics, and\n",
    "           List[1] contains the (title, artist) tuples that were not found'''\n",
    "  \n",
    "    # Format data so that it works in the url\n",
    "    target_list = []\n",
    "    replacements = {' ':'_', '(':'', ')':'', '&':'and', \"'\":\"\", '?':''}\n",
    "    for n in range(len(combined_list)):\n",
    "        target = ''.join((combined_list[n][1], \":\", combined_list[n][0]))\n",
    "        for a,b in replacements.items():\n",
    "            target = target.replace(a,b)\n",
    "        target_list.append(target)\n",
    "    \n",
    "    # Now grab the lyrics for all songs in the year, but store any songs for which\n",
    "    # the formatting is incorrect in a separate list\n",
    "    \n",
    "    failures = []\n",
    "    lyrics = []\n",
    "    for n in range(len(target_list)):\n",
    "        lyrics_page = 'http://www.xxxxxxxxxx.com/wiki/{}'.format(target_list[n].lower())\n",
    "        time.sleep(1) # put in a delay so that we don't hammer their website too badly\n",
    "        try:\n",
    "            page2 = urllib.request.urlopen(lyrics_page)\n",
    "            parsed2 = BeautifulSoup(page2, 'html.parser')\n",
    "            # format needs a bit more processing than before\n",
    "            x = parsed2.findAll(\"div\", class_=\"lyricbox\")[0]\n",
    "            y = ' '.join(str(child) for child in x.children)\n",
    "            y = y.replace(\"<br/>\", \".\")\n",
    "            y = y.split('<div class=\"lyricsbreak\"></div> \\n')[0]\n",
    "            lyrics.append(y)\n",
    "        except:\n",
    "            failures.append(combined_list[n])\n",
    "        \n",
    "    all_lyrics = \" \".join(lyrics).replace('\\n','. ')\n",
    "    result = [all_lyrics, failures]\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this process with different sites until substantially all the lyrics have been collected. At each pass, check the failures and search manually on the site to understand what the formatting issue was, then correct it and add the additional lyrics to the master dictionary. A common problem is collaborations, which were particularly popular in the 1990's and early 2000's. The following code may help  \n",
    "  \n",
    "```\n",
    "    song_delimiters = \"FT | FEATURING | VS | WITH \"\n",
    "    artist_delimiters = \"FT | FEATURING | VS | WITH | AND\"\n",
    "    for n, item in enumerate(combined_list):\n",
    "        song = re.split(song_delimiters, item[0])[0]\n",
    "        artist = re.split(artist_delimiters, item[1])[0]\n",
    "        combined_list[n] = [song,artist]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, combine the UK data with the US data, into a single corpus of song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get correct encoding of csv file in terminal, using the command 'file billboard_lyrics_1964-2015.csv'\n",
    "df = pd.read_csv('billboard_lyrics_1964-2015.csv', encoding='ISO-8859-1').set_index(\"Year\")\n",
    "df = df.drop([\"Rank\", \"Song\", \"Artist\", \"Source\"], axis = 1)\n",
    "\n",
    "# Get the data into a structure that is consistent with how the UK data is stored\n",
    "# First, set up a dictionary where the key is year, and the value is a list of song\n",
    "# lyrics for the year\n",
    "us_dict = defaultdict(list)\n",
    "for year, row in df.iterrows():\n",
    "    us_dict[str(year)].append(str(row[\"Lyrics\"]))\n",
    "    \n",
    "# Then collapse the list of lyrics into a single string for each year, and save\n",
    "# the dictionary in case we want to do analysis across time periods\n",
    "for k, v in us_dict.items():\n",
    "    us_dict[k] = \" \".join((v))    \n",
    "with open('US_lyric_dict_final.txt', 'w') as file:\n",
    "     file.write(json.dumps(us_dict))\n",
    "        \n",
    "# Finally, create a single file of text, save it...\n",
    "with open(\"US_lyrics_final.txt\", 'w') as file:\n",
    "    file.write(\" \".join(us_dict[year] for year in us_dict.keys()))\n",
    "\n",
    "# ...and merge it with the UK file\n",
    "with open('Combined_lyrics_final.txt', 'w') as output:\n",
    "    with open('US_lyrics_final.txt') as temp:\n",
    "        for line in temp:\n",
    "            output.write(line)\n",
    "    with open('UK_lyrics_final.txt') as temp:\n",
    "        for line in temp:\n",
    "            output.write(line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
