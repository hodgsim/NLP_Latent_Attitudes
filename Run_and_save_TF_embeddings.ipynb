{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and save datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step that tokenizes the data can be fairly time consuming, and since the analysis has been broken up across multiple workbooks, it makes sense to load the raw data, process it once and save it in processed processed form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonhodgkinson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import custom_embedding_functions as embed\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Female_Authors: \n",
      "Document contains  44455 distinct words\n",
      "Total word count =  1804152\n",
      "\n",
      "Male_Authors: \n",
      "Document contains  55732 distinct words\n",
      "Total word count =  2207190\n",
      "\n",
      "Movie_Scripts: \n",
      "Document contains  74946 distinct words\n",
      "Total word count =  5350737\n",
      "\n",
      "Combined_lyrics_final: \n",
      "Document contains  47064 distinct words\n",
      "Total word count =  1301368\n"
     ]
    }
   ],
   "source": [
    "text_data = ['Female_Authors', 'Male_Authors', 'Movie_Scripts', 'Combined_lyrics_final']\n",
    "\n",
    "for dataset in text_data:\n",
    "    full_text = []\n",
    "    vocab_size = []\n",
    "    clean = ''\n",
    "    \n",
    "    # tokenize document and save tokenized text\n",
    "    with open('../Data/'+dataset+'.txt', 'r') as file:\n",
    "        clean += file.read().lower()\n",
    "        clean = re.sub(\"[^a-z]\",\" \",clean)\n",
    "        print('\\n'+dataset+': ')\n",
    "        full_text, vocab_size = embed.tokenize(clean, exclude_stopwords=True)\n",
    "    \n",
    "    with open('../Data/'+dataset+'_tokenized.json', 'w') as file:\n",
    "        json.dump(full_text, file)\n",
    "    \n",
    "    # process tokenized document and save key information\n",
    "    data, count, dictionary, reversed_dictionary = embed.build_dataset(full_text, vocab_size)\n",
    "\n",
    "    structure = ['data','count','dictionary','reversed_dictionary']\n",
    "    processed_dataset = {listname:globals()[listname] for listname in structure}\n",
    "    \n",
    "    with open('../Data/'+dataset+'_processed.json', 'w') as file:\n",
    "        json.dump(processed_dataset, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can now be retrieved by loading the json file and assigning the various datasets to their corresponding variable names, eg  \n",
    "```python\n",
    "\n",
    "with open('Female_Authors_processed.json', 'r') as file:  \n",
    "    processed_dataset = json.load(file)  \n",
    "         \n",
    "data = processed_dataset.get('data')\n",
    "count = processed_dataset.get('count')\n",
    "dictionary = processed_dataset.get('dictionary')\n",
    "reversed_dictionary = processed_dataset.get('reversed_dictionary')\n",
    "del processed_dataset```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
