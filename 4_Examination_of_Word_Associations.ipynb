{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between datasets\n",
    "### Part two:    Word Associations  \n",
    "  \n",
    "An established method for uncovering hidden bias is to use the **Implicit Association Test (IAT)**. The test is administered by showing the subject a series of words relating to a specific concept, and asking them to sort them into categories that are paired with a specific attribute. For example, they may be shown types of flowers and types of insects, and asked to categorize them into pairings that include either pleasant words or unpleasant words. By pairing the target concepts (flowers and insects) with the attribute words (pleasant and unpleasant) and measuring the response time to sort pairings, it is possible to uncover hidden attitudes. In this example, it is almost always the case that people can sort words into categories more quickly when flowers are paired with pleasant words, and insects are paired with unpleasant words. Response times slow dramatically when the pairings are reversed, so that it takes much longer to determine the correct category of \"cockroach\" if the options are pleasant/insect or unpleasant/flower. Read more [here](https://implicit.harvard.edu/implicit/education.html) and **take the test [here](https://implicit.harvard.edu/implicit/takeatest.html)**.  \n",
    "  \n",
    "In a [paper](../Papers/Caliskan%202017%20-%20Semantics%20derived%20automatically%20from%20language%20corpora%20contain%20human-like%20biases.pdf) from 2017, Caliskan and her team showed that commonly observed biases in the IAT test could also be observed in gloVe word embeddings, by looking at cosine similarity between vector pairs.\n",
    "\n",
    "Depending upon the dataset chosen, gloVe embeddings are derived from Wikipedia, Twitter or Common Crawl, and therefore reflect broadly occuring bias among a vast range of authors. In this exercise, we will use the WEAT (word embedding association test) developed by Caliskan, but apply it to specific domains to see whether more narrowly held biases are revealed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PESlgxG5_APe"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VolEl6ptMPYW"
   },
   "outputs": [],
   "source": [
    "# Load the embedding vectors that were just calculated\n",
    "female_vectors = KeyedVectors.load('../Data/female_model.wv', mmap='r')\n",
    "male_vectors = KeyedVectors.load('../Data/male_model.wv', mmap='r')\n",
    "movie_vectors = KeyedVectors.load('../Data/movie_model.wv', mmap='r')\n",
    "lyrics_vectors = KeyedVectors.load('../Data/lyrics_model.wv', mmap='r')\n",
    "\n",
    "# Set up lists of words used in IAT\n",
    "flowers = ['hyacinth', 'marigold', 'poppy', 'azalea', 'crocus', 'iris', 'orchid', 'rose', 'bluebell', 'daffodil', 'lilac', 'pansy', 'tulip', 'buttercup', 'daisy', 'lily', 'peony', 'violet', 'carnation', 'gladiola', 'magnolia', 'petunia', 'zinnia']\n",
    "insects = ['ant', 'caterpillar', 'flea', 'locust', 'spider', 'bedbug', 'centipede', 'maggot', 'tarantula', 'bee', 'cockroach', 'gnat', 'mosquito', 'termite', 'cricket', 'hornet', 'moth', 'wasp', 'blackfly', 'dragonfly', 'horsefly', 'roach', 'weevil']\n",
    "pleasant = ['caress', 'freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond', 'gentle', 'honest', 'lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle', 'sunrise', 'family', 'happy', 'laughter', 'paradise', 'vacation']\n",
    "unpleasant = ['abuse', 'crash', 'filth', 'murder', 'sickness', 'accident','death', 'grief', 'poison', 'stink', 'assault', 'disaster', 'hatred', 'pollute', 'tragedy','bomb', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'evil', 'kill', 'rotten', 'vomit','agony', 'prison']\n",
    "instruments = ['bagpipe','cello','guitar','lute','trombone','banjo','clarinet','harmonica','mandolin','trumpet','bassoon','drum','harp','oboe','tuba','bell','fiddle','harpsichord','piano','viola','bongo','flute','horn','saxophone','violin']\n",
    "weapons = ['arrow', 'club', 'gun', 'missile', 'spear', 'axe', 'dagger', 'harpoon', 'pistol', 'sword', 'blade', 'dynamite', 'hatchet', 'rifle', 'tank', 'bomb', 'firearm', 'knife', 'shotgun', 'teargas', 'cannon', 'grenade', 'mace', 'slingshot', 'whip']\n",
    "male_names = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
    "female_names = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
    "career = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n",
    "family = ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']\n",
    "maths = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "science = ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy']\n",
    "arts = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "male_terms = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n",
    "female_terms = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n",
    "mental_disease = ['sad', 'hopeless', 'gloomy', 'tearful', 'miserable', 'depressed']\n",
    "physical_disease = ['sick', 'illness', 'influenza', 'disease', 'virus', 'cancer']\n",
    "temporary = ['impermanent', 'unstable', 'variable', 'fleeting', 'short-term', 'brief', 'occasional']\n",
    "permanent = ['stable', 'always', 'constant', 'persistent', 'chronic', 'prolonged', 'forever']\n",
    "freedom = ['capitalism', 'freedom', 'autonomy', 'independence', 'liberty', 'sovereignty', 'license', 'choice', 'different']\n",
    "equality = ['socialism', 'equity', 'fairness', 'coequality', 'equivalence', 'parity', 'equal', 'impartiality', 'same']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WEAT test involve looking at the cosine similarity between pairs of word embeddings.  \n",
    "  \n",
    "If the author holds no implicit bias, then it is reasonable to assume that there should be no difference in the cosine similarity between either set of target words and the attribute words. This is because the embeddings are derived from the written works of the author, and therefore reflect the way that they use language and the words that they choose when describing situations, characters or concepts.  \n",
    "  \n",
    "For example, we could test for gender bias by selecting a target set of scientific words and artistic words, and then calculating the average cosine similarity versus sets of male and female words. If the author does not associate male  with science and female with arts, then there should be no statistical difference between the two means.  \n",
    "  \n",
    "To determine statistical significance, we'll use **Cohen's d**, which is a standard measure of effect size when comparing two population means.  \n",
    "  \n",
    "Cohen's d = $\\frac{\\bar{x_1}-\\bar{x_2}}{sd_{pooled}}$  \n",
    "  \n",
    "Typically effect sizes of 0.2, 0.5 and 0.8 are considered small, medium and large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(target1, target2, attr1, attr2, model, verbose=False):\n",
    "    \"\"\"Calculates cosine similarity between the embeddings for two sets of target words versus two sets of \n",
    "       attribute words, takes the difference, and returns the Cohen's d statistic as a measure of effect size.\n",
    "       \n",
    "       Args:\n",
    "           target1, target2: sets of concept words to be tested for bias\n",
    "           attr1, attr2: sets of attributes representing the bias being tested for\n",
    "           model: (gensim.KeyedVectors object) word embeddings to use\n",
    "           verbose: optional flag to return further detail for investigating the drivers of any difference\n",
    "       Returns:\n",
    "           Cohen's d statistic: 0.2 is a small effect, 0.5 is medium and 0.8 is large\n",
    "           Swab_x_detail, swab_y_detail (if verbose is set to True): list of tuples, showing each target word \n",
    "           and the mean association versus attribute1 and attribute2\"\"\"\n",
    "    \n",
    "    # First, ensure that the words are actually in the vocab\n",
    "    check1 = [word for word in target1 if word in model.vocab]\n",
    "    check2 = [word for word in target2 if word in model.vocab]\n",
    "    check3 = [word for word in attr1 if word in model.vocab]\n",
    "    check4 = [word for word in attr2 if word in model.vocab]\n",
    "    \n",
    "    # Then, cap the length of each pair of lists so that the two are equal\n",
    "    if len(check1)>len(check2):\n",
    "        check1 = check1[:len(check2)]\n",
    "    else:\n",
    "        check2 = check2[:len(check1)]\n",
    "        \n",
    "    if len(check3)>len(check4):\n",
    "        check3 = check3[:len(check4)]\n",
    "    else:\n",
    "        check4 = check4[:len(check3)]\n",
    "    \n",
    "    # Revise inputs so that they contain only words in the vocab and they are symmetrical\n",
    "    target1, target2, attr1, attr2 = check1, check2, check3, check4\n",
    "    \n",
    "    # Calc difference in mean similarity vs each set of attributes for target group 1\n",
    "    swab_x = []\n",
    "    swab_x_detail = []\n",
    "    for word in target1:\n",
    "        cos_sim_a = [model.similarity(word,i) for i in attr1]\n",
    "        cos_sim_b = [model.similarity(word,j) for j in attr2]\n",
    "        diff = np.mean(cos_sim_a)-np.mean(cos_sim_b)\n",
    "        swab_x.append(diff)\n",
    "        swab_x_detail.append((word, np.mean(cos_sim_a), np.mean(cos_sim_b)))\n",
    "    \n",
    "    # Calc difference in mean similarity vs each set of attributes for target group 2\n",
    "    swab_y = []\n",
    "    swab_y_detail = []\n",
    "    for word in target2:\n",
    "        cos_sim_a = [model.similarity(word,i) for i in attr1]\n",
    "        cos_sim_b = [model.similarity(word,j) for j in attr2]\n",
    "        diff = np.mean(cos_sim_a)-np.mean(cos_sim_b)\n",
    "        swab_y.append(diff)\n",
    "        swab_y_detail.append((word, np.mean(cos_sim_a), np.mean(cos_sim_b)))\n",
    "    \n",
    "    # Calculate difference in mean similarity for the pooled group of target words\n",
    "    # vs each set of attributes. The standard deviation of all differences will be \n",
    "    # used as a scaling factor when calculating cohen's d statistic\n",
    "    swab_z = []\n",
    "    for word in target1+target2:\n",
    "        cos_sim_a = [model.similarity(word,i) for i in attr1]\n",
    "        cos_sim_b = [model.similarity(word,j) for j in attr2]\n",
    "        diff = np.mean(cos_sim_a)-np.mean(cos_sim_b)\n",
    "        swab_z.append(diff)\n",
    "    \n",
    "    # Cohen's d = difference in means divided by th epooled standard deviation\n",
    "    mean_diff = np.mean(swab_x)-np.mean(swab_y)\n",
    "    std = np.std(swab_z)\n",
    "    \n",
    "    if verbose:\n",
    "        return mean_diff/std, swab_x_detail, swab_y_detail\n",
    "    else:\n",
    "        return mean_diff/std\n",
    "\n",
    "def view_detail(target1, target2, attr1, attr2, model):\n",
    "    \"\"\"Runs verbose version of Cohen D function above, and returns dataframe\n",
    "       showing detail of net scores\n",
    "       \n",
    "       Args:target1, target2: sets of concept words to be tested for bias\n",
    "            attr1, attr2: sets of attributes representing the bias being tested for\n",
    "            model: (gensim.KeyedVectors object) word embeddings to use\n",
    "       Returns: Pandas dataframe containing each word, the average similarity versus\n",
    "            attr1 (labelled A), attr2 (labelled B) and the net score\n",
    "         \"\"\"\n",
    "    score,a,b = cohen_d(target1, target2, attr1, attr2, model, True)\n",
    "    df = pd.DataFrame(a,b)\n",
    "    # turn index (tuple of word, value1, value2) into a column that we'll call temp\n",
    "    df.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    # assign column names\n",
    "    df.columns=['temp','target1', 'A1', 'B1']\n",
    "    \n",
    "    # split temp tuple into component columns\n",
    "    df['target2'], df['A2'], df['B2'] = df['temp'].str\n",
    "    \n",
    "    # calculate net attribute score for each word\n",
    "    df['net_A1'] = df['A1'] - df['B1']\n",
    "    df['net_A2'] = df['A2'] - df['B2']\n",
    "    \n",
    "    # remove temp column\n",
    "    df = df.drop(['temp'], axis=1)\n",
    "    \n",
    "    # reorder columns\n",
    "    df = df[['target1', 'A1', 'B1','net_A1',\n",
    "             'target2', 'A2', 'B2','net_A2']]\n",
    "    \n",
    "    print('Cohens d: {0:.2f}'.format(score))\n",
    "    print('Average net A1: {0:.2f} \\nAverage net A2: {1:.2f}'.format(np.mean(df['net_A1']), np.mean(df['net_A2'])))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing  \n",
    "  \n",
    "The first step is to download pre-trained gloVe embeddings from the Stanford NLP site, [here](https://nlp.stanford.edu/projects/glove/), and use these to recreate Caliskan's results. This step isn't shown in this workbook, because the method is the same as below but the associated data file is very large. Instead we'll just present results using our bespoke trained embeddings and look for interesting associations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Flowers vs Insects    \n",
    "  \n",
    "This is a classic IAT test and a well established relationship, where flowers are generally seen as more pleasant than insects. We'll see how well this is reflected in our datasets;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Authors: 0.15\n",
      "Male Authors: -0.01\n",
      "Movie Scripts: 0.78\n",
      "Song Lyrics: 0.73\n"
     ]
    }
   ],
   "source": [
    "print(\"Female Authors: {:.2f}\".format(cohen_d(flowers, insects, pleasant, unpleasant, female_vectors)))\n",
    "print(\"Male Authors: {:.2f}\".format(cohen_d(flowers, insects, pleasant, unpleasant, male_vectors)))\n",
    "print(\"Movie Scripts: {:.2f}\".format(cohen_d(flowers, insects, pleasant, unpleasant, movie_vectors)))\n",
    "print(\"Song Lyrics: {:.2f}\".format(cohen_d(flowers, insects, pleasant, unpleasant, lyrics_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship is visible in all data apart from the works of male authors. If we look at the cosine similarity scores for male authors in more detail;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens d: -0.01\n",
      "Average net A1: -0.11 \n",
      "Average net A2: -0.11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target1</th>\n",
       "      <th>A1</th>\n",
       "      <th>B1</th>\n",
       "      <th>net_A1</th>\n",
       "      <th>target2</th>\n",
       "      <th>A2</th>\n",
       "      <th>B2</th>\n",
       "      <th>net_A2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyacinth</td>\n",
       "      <td>0.262470</td>\n",
       "      <td>0.424541</td>\n",
       "      <td>-0.162071</td>\n",
       "      <td>ant</td>\n",
       "      <td>0.307444</td>\n",
       "      <td>0.477451</td>\n",
       "      <td>-0.170007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poppy</td>\n",
       "      <td>0.253178</td>\n",
       "      <td>0.354917</td>\n",
       "      <td>-0.101738</td>\n",
       "      <td>caterpillar</td>\n",
       "      <td>0.362725</td>\n",
       "      <td>0.476503</td>\n",
       "      <td>-0.113778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iris</td>\n",
       "      <td>0.320511</td>\n",
       "      <td>0.448017</td>\n",
       "      <td>-0.127506</td>\n",
       "      <td>flea</td>\n",
       "      <td>0.354805</td>\n",
       "      <td>0.434885</td>\n",
       "      <td>-0.080080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orchid</td>\n",
       "      <td>0.258529</td>\n",
       "      <td>0.364524</td>\n",
       "      <td>-0.105995</td>\n",
       "      <td>locust</td>\n",
       "      <td>0.271611</td>\n",
       "      <td>0.416257</td>\n",
       "      <td>-0.144646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rose</td>\n",
       "      <td>0.070141</td>\n",
       "      <td>0.085615</td>\n",
       "      <td>-0.015473</td>\n",
       "      <td>spider</td>\n",
       "      <td>0.304322</td>\n",
       "      <td>0.419675</td>\n",
       "      <td>-0.115352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bluebell</td>\n",
       "      <td>0.326930</td>\n",
       "      <td>0.447678</td>\n",
       "      <td>-0.120748</td>\n",
       "      <td>centipede</td>\n",
       "      <td>0.280367</td>\n",
       "      <td>0.319470</td>\n",
       "      <td>-0.039104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lilac</td>\n",
       "      <td>0.295143</td>\n",
       "      <td>0.436254</td>\n",
       "      <td>-0.141112</td>\n",
       "      <td>maggot</td>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.440828</td>\n",
       "      <td>-0.110320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pansy</td>\n",
       "      <td>0.137049</td>\n",
       "      <td>0.281499</td>\n",
       "      <td>-0.144450</td>\n",
       "      <td>tarantula</td>\n",
       "      <td>0.169128</td>\n",
       "      <td>0.281134</td>\n",
       "      <td>-0.112006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tulip</td>\n",
       "      <td>0.263616</td>\n",
       "      <td>0.382331</td>\n",
       "      <td>-0.118714</td>\n",
       "      <td>bee</td>\n",
       "      <td>0.339156</td>\n",
       "      <td>0.477989</td>\n",
       "      <td>-0.138833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>daisy</td>\n",
       "      <td>0.133131</td>\n",
       "      <td>0.126421</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>gnat</td>\n",
       "      <td>0.327698</td>\n",
       "      <td>0.375513</td>\n",
       "      <td>-0.047815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lily</td>\n",
       "      <td>0.301721</td>\n",
       "      <td>0.469672</td>\n",
       "      <td>-0.167951</td>\n",
       "      <td>mosquito</td>\n",
       "      <td>0.268943</td>\n",
       "      <td>0.387924</td>\n",
       "      <td>-0.118981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>violet</td>\n",
       "      <td>0.300096</td>\n",
       "      <td>0.446901</td>\n",
       "      <td>-0.146805</td>\n",
       "      <td>cricket</td>\n",
       "      <td>0.279259</td>\n",
       "      <td>0.406386</td>\n",
       "      <td>-0.127128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>carnation</td>\n",
       "      <td>0.353086</td>\n",
       "      <td>0.434960</td>\n",
       "      <td>-0.081874</td>\n",
       "      <td>hornet</td>\n",
       "      <td>0.148664</td>\n",
       "      <td>0.193640</td>\n",
       "      <td>-0.044976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>magnolia</td>\n",
       "      <td>0.266921</td>\n",
       "      <td>0.339560</td>\n",
       "      <td>-0.072639</td>\n",
       "      <td>moth</td>\n",
       "      <td>0.133107</td>\n",
       "      <td>0.261478</td>\n",
       "      <td>-0.128371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target1        A1        B1    net_A1      target2        A2        B2  \\\n",
       "0    hyacinth  0.262470  0.424541 -0.162071          ant  0.307444  0.477451   \n",
       "1       poppy  0.253178  0.354917 -0.101738  caterpillar  0.362725  0.476503   \n",
       "2        iris  0.320511  0.448017 -0.127506         flea  0.354805  0.434885   \n",
       "3      orchid  0.258529  0.364524 -0.105995       locust  0.271611  0.416257   \n",
       "4        rose  0.070141  0.085615 -0.015473       spider  0.304322  0.419675   \n",
       "5    bluebell  0.326930  0.447678 -0.120748    centipede  0.280367  0.319470   \n",
       "6       lilac  0.295143  0.436254 -0.141112       maggot  0.330508  0.440828   \n",
       "7       pansy  0.137049  0.281499 -0.144450    tarantula  0.169128  0.281134   \n",
       "8       tulip  0.263616  0.382331 -0.118714          bee  0.339156  0.477989   \n",
       "9       daisy  0.133131  0.126421  0.006710         gnat  0.327698  0.375513   \n",
       "10       lily  0.301721  0.469672 -0.167951     mosquito  0.268943  0.387924   \n",
       "11     violet  0.300096  0.446901 -0.146805      cricket  0.279259  0.406386   \n",
       "12  carnation  0.353086  0.434960 -0.081874       hornet  0.148664  0.193640   \n",
       "13   magnolia  0.266921  0.339560 -0.072639         moth  0.133107  0.261478   \n",
       "\n",
       "      net_A2  \n",
       "0  -0.170007  \n",
       "1  -0.113778  \n",
       "2  -0.080080  \n",
       "3  -0.144646  \n",
       "4  -0.115352  \n",
       "5  -0.039104  \n",
       "6  -0.110320  \n",
       "7  -0.112006  \n",
       "8  -0.138833  \n",
       "9  -0.047815  \n",
       "10 -0.118981  \n",
       "11 -0.127128  \n",
       "12 -0.044976  \n",
       "13 -0.128371  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_detail(flowers, insects, pleasant, unpleasant, male_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that while flowers have some association with pleasant words, their association with unpleasant words is actually stronger. That means that although there is a net unpleasant score for insects (as with the other datasets) there is also a net unpleasant score for flowers, which leads to the low Cohen's d score (indicating no significant difference in the level of unpleasantness between flowers and insects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Maths vs Arts  \n",
    "  \n",
    "We'll check these targets for gender bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Authors: -0.54\n",
      "Male Authors: 0.02\n",
      "Movie Scripts: 0.17\n",
      "Song Lyrics: -0.52\n"
     ]
    }
   ],
   "source": [
    "print(\"Female Authors: {:.2f}\".format(cohen_d(maths, arts, male_terms, female_terms, female_vectors)))\n",
    "print(\"Male Authors: {:.2f}\".format(cohen_d(maths, arts, male_terms, female_terms, male_vectors)))\n",
    "print(\"Movie Scripts: {:.2f}\".format(cohen_d(maths, arts, male_terms, female_terms, movie_vectors)))\n",
    "print(\"Song Lyrics: {:.2f}\".format(cohen_d(maths, arts, male_terms, female_terms, lyrics_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't significant gender differences in the way that math or art is described in either the works of male authors or movie scripts. However, female authors and lyricists both associate female nouns and pronouns (woman, girl, she, her etc) more strongly with the arts than with mathematics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Mental vs Physical disease  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Authors: -1.55\n",
      "Male Authors: -1.46\n",
      "Movie Scripts: -1.50\n",
      "Song Lyrics: -0.15\n"
     ]
    }
   ],
   "source": [
    "print(\"Female Authors: {:.2f}\".format(cohen_d(mental_disease, physical_disease, male_terms, female_terms, female_vectors)))\n",
    "print(\"Male Authors: {:.2f}\".format(cohen_d(mental_disease, physical_disease, male_terms, female_terms, male_vectors)))\n",
    "print(\"Movie Scripts: {:.2f}\".format(cohen_d(mental_disease, physical_disease, male_terms, female_terms, movie_vectors)))\n",
    "print(\"Song Lyrics: {:.2f}\".format(cohen_d(mental_disease, physical_disease, male_terms, female_terms, lyrics_vectors)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datasets (apart from lyrics) associate female terms much more strongly with mental illness than they do for male terms. Physical illness is more evenly balanced, but is slightly more likely to relate to females.  \n",
    "  \n",
    "We can also look at how diseases are expected to persist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Authors: 0.83\n",
      "Male Authors: -0.38\n",
      "Movie Scripts: 1.17\n",
      "Song Lyrics: -1.30\n"
     ]
    }
   ],
   "source": [
    "print(\"Female Authors: {:.2f}\".format(cohen_d(mental_disease, physical_disease, temporary, permanent, female_vectors)))\n",
    "print(\"Male Authors: {:.2f}\".format(cohen_d(mental_disease, physical_disease, temporary, permanent, male_vectors)))\n",
    "print(\"Movie Scripts: {:.2f}\".format(cohen_d(mental_disease, physical_disease, temporary, permanent, movie_vectors)))\n",
    "print(\"Song Lyrics: {:.2f}\".format(cohen_d(mental_disease, physical_disease, temporary, permanent, lyrics_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Female authors view words in the mental disease set as being temporary afflictions, whereas male authors show no difference in their associations between mental disease and duration. They do however show a stronger tendency to describe physical illness as permanent.\n",
    "\n",
    "Movie scripts portray physical illness as permanent, while song lyrics strongly associate both physical and mental illness as being temporary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohens d: 0.83\n",
      "Average net A1: 0.09 \n",
      "Average net A2: 0.03\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target1</th>\n",
       "      <th>A1</th>\n",
       "      <th>B1</th>\n",
       "      <th>net_A1</th>\n",
       "      <th>target2</th>\n",
       "      <th>A2</th>\n",
       "      <th>B2</th>\n",
       "      <th>net_A2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad</td>\n",
       "      <td>0.330556</td>\n",
       "      <td>0.169816</td>\n",
       "      <td>0.160740</td>\n",
       "      <td>sick</td>\n",
       "      <td>0.149597</td>\n",
       "      <td>0.174408</td>\n",
       "      <td>-0.024811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hopeless</td>\n",
       "      <td>0.467321</td>\n",
       "      <td>0.357552</td>\n",
       "      <td>0.109769</td>\n",
       "      <td>illness</td>\n",
       "      <td>0.229543</td>\n",
       "      <td>0.264330</td>\n",
       "      <td>-0.034787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gloomy</td>\n",
       "      <td>0.572050</td>\n",
       "      <td>0.366900</td>\n",
       "      <td>0.205150</td>\n",
       "      <td>influenza</td>\n",
       "      <td>0.443839</td>\n",
       "      <td>0.359768</td>\n",
       "      <td>0.084070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tearful</td>\n",
       "      <td>0.505256</td>\n",
       "      <td>0.466448</td>\n",
       "      <td>0.038807</td>\n",
       "      <td>disease</td>\n",
       "      <td>0.510107</td>\n",
       "      <td>0.433572</td>\n",
       "      <td>0.076534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miserable</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>0.169180</td>\n",
       "      <td>-0.052462</td>\n",
       "      <td>virus</td>\n",
       "      <td>0.429896</td>\n",
       "      <td>0.392743</td>\n",
       "      <td>0.037153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>depressed</td>\n",
       "      <td>0.484364</td>\n",
       "      <td>0.400152</td>\n",
       "      <td>0.084213</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.510517</td>\n",
       "      <td>0.472274</td>\n",
       "      <td>0.038243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target1        A1        B1    net_A1    target2        A2        B2  \\\n",
       "0        sad  0.330556  0.169816  0.160740       sick  0.149597  0.174408   \n",
       "1   hopeless  0.467321  0.357552  0.109769    illness  0.229543  0.264330   \n",
       "2     gloomy  0.572050  0.366900  0.205150  influenza  0.443839  0.359768   \n",
       "3    tearful  0.505256  0.466448  0.038807    disease  0.510107  0.433572   \n",
       "4  miserable  0.116718  0.169180 -0.052462      virus  0.429896  0.392743   \n",
       "5  depressed  0.484364  0.400152  0.084213     cancer  0.510517  0.472274   \n",
       "\n",
       "     net_A2  \n",
       "0 -0.024811  \n",
       "1 -0.034787  \n",
       "2  0.084070  \n",
       "3  0.076534  \n",
       "4  0.037153  \n",
       "5  0.038243  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_detail(mental_disease, physical_disease, temporary, permanent, female_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Freedom versus equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female Authors: 0.79\n",
      "Male Authors: 0.53\n",
      "Movie Scripts: 1.33\n",
      "Song Lyrics: 0.24\n"
     ]
    }
   ],
   "source": [
    "print(\"Female Authors: {:.2f}\".format(cohen_d(freedom, equality, male_terms, female_terms, female_vectors)))\n",
    "print(\"Male Authors: {:.2f}\".format(cohen_d(freedom, equality, male_terms, female_terms, male_vectors)))\n",
    "print(\"Movie Scripts: {:.2f}\".format(cohen_d(freedom, equality, male_terms, female_terms, movie_vectors)))\n",
    "print(\"Song Lyrics: {:.2f}\".format(cohen_d(freedom, equality, male_terms, female_terms, lyrics_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In add datasets, the concept of freedom is more stronly associated with male terms than with female terms, however the effect is strongest in the works of female authors and in movie scripts"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WEAT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
